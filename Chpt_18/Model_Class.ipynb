{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0173cd05-b1f6-46bf-8aff-e833bb685dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568caff-b6b9-46f2-aeea-3ae1e3fb836b",
   "metadata": {},
   "source": [
    "## Updated Layer Dense class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c94a93-ea03-47be-a776-43044f5efb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_inputs,\n",
    "        n_neurons,\n",
    "        weight_regularizer_l1=0,\n",
    "        weight_regularizer_l2=0,\n",
    "        bias_regularizer_l1=0,\n",
    "        bias_regularizer_l2=0,\n",
    "    ):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd746f8-ff50-49d8-9061-06c4c59384c0",
   "metadata": {},
   "source": [
    "## Input layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481ca13b-281d-4216-a1cf-7703bf22d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Input:\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85190220-53ce-4ee1-a97a-94c14833ee05",
   "metadata": {},
   "source": [
    "## Updated Dropout layer class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5018dc2-c04b-4bab-ab2f-59c0bcd5d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "\n",
    "        self.binary_mask = (\n",
    "            np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        )\n",
    "\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95672e5c-721f-4ad6-8ad1-b76b6f5dd1bc",
   "metadata": {},
   "source": [
    "## Updated Common Loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d266e3f-9dc8-4e42-a997-fcd95c8041f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def regularization_loss(self):\n",
    "        regularization_loss = 0\n",
    "\n",
    "        for layer in self.trainable_layers:\n",
    "\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(\n",
    "                    np.abs(layer.weights)\n",
    "                )\n",
    "\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(\n",
    "                    layer.weights * layer.weights\n",
    "                )\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(\n",
    "                    np.abs(layer.biases)\n",
    "                )\n",
    "\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(\n",
    "                    layer.biases * layer.biases\n",
    "                )\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        return data_loss, self.regularization_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd56f16-703d-4a91-9b8c-b0f76f8e0e6c",
   "metadata": {},
   "source": [
    "## Common Accuracy class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d480a63a-b0ec-4c81-bc26-3e69d58ac094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def calculate(self, predictions, y):\n",
    "        comparisons = self.compare(predictions, y)\n",
    "\n",
    "        accuracy = np.mean(comparisons)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ac283-830c-4e33-8ff4-23265876e863",
   "metadata": {},
   "source": [
    "## Regression Accuracy class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436fdfcc-6364-4448-a301-02fdf6430197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Regression(Accuracy):\n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ce735-2ef1-43b5-8a8a-613ce9b73595",
   "metadata": {},
   "source": [
    "## Categorical Accuracy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c493c2-aad1-4708-9032-680edcff4eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy_Categorical(Accuracy):\n",
    "    def init(self, y):\n",
    "        pass\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        #         if len(y.shape) == 2:\n",
    "        #             y = np.argmax(y, axis=1)\n",
    "        return predictions == y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffda0c-d92f-4ee3-a4e4-7cbe502f7f07",
   "metadata": {},
   "source": [
    "## Mean Absolute Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daad6282-1135-4bce-ac92-8b91b6c93e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAbsoluteError_Loss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd83cdbb-28c7-4cad-86b3-06fc1e074d93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mean Squared Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b170b50-d417-4087-8cc1-5fe22078d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError_Loss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        sample_losses = np.mean((y_true - y_pred) ** 2, axis=-1)\n",
    "\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7956cf-8dc0-4521-a4ed-943e4e105b52",
   "metadata": {},
   "source": [
    "## Categorical Crossentropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41de89fa-873d-4884-9369-14d2461a7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossentropy_Loss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44435e-5f51-4467-a0d2-905b5fcebfb5",
   "metadata": {},
   "source": [
    "## Activation Softmax & Categorical Crossentropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d89a3f79-3f19-4397-869d-cbfa942e112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Activation_Functions import Activation_Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d489fae1-5674-46d2-919f-ee1fd3fb4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "#     def __init__(self):\n",
    "#         self.activation = Activation_Softmax()\n",
    "#         self.loss = CategoricalCrossentropy_Loss()\n",
    "\n",
    "#     def forward(self, inputs, y_true):\n",
    "#         self.activation.forward(inputs)\n",
    "#         self.output = self.activation.output\n",
    "#         return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbff5d2-bea9-4f1d-9ca7-d63da5b39937",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a472883-84d0-4136-bdb5-4b2ee6c5325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossentropy_Loss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        sample_losses = -(\n",
    "            y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped)\n",
    "        )\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        self.dinputs = (\n",
    "            -(y_true / clipped_dvalues - (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        )\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd23faa-98be-4da0-93bd-a3cac31343f9",
   "metadata": {},
   "source": [
    "# Model Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6290f23e-6c45-4506-8eb7-5618a314317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(\n",
    "            self.loss, CategoricalCrossentropy_Loss\n",
    "        ):\n",
    "            self.softmax_classifier_output = (\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "            )\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        return layer.output\n",
    "\n",
    "    def backward(self, output, y):\n",
    "        if self.softmax_classifier_output is not None:\n",
    "\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "    def train(self, X, y, *, epochs=1, print_every=1, validation_data=None):\n",
    "\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            output = self.forward(X, training=True)\n",
    "\n",
    "            data_loss, regularization_loss = self.loss.calculate(\n",
    "                output, y, include_regularization=True\n",
    "            )\n",
    "            loss = data_loss + regularization_loss\n",
    "\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y)\n",
    "\n",
    "            self.backward(output, y)\n",
    "\n",
    "            self.optimizer.pre_update_params()\n",
    "            for layer in self.trainable_layers:\n",
    "                self.optimizer.update_params(layer)\n",
    "            self.optimizer.post_update_params()\n",
    "\n",
    "            if not epoch % print_every:\n",
    "                print(\n",
    "                    f\"epoch: {epoch}, \"\n",
    "                    + f\"acc: {accuracy:.3f}, \"\n",
    "                    + f\"loss: {loss:.3f} (\"\n",
    "                    + f\"data_loss: {data_loss:.3f}, \"\n",
    "                    + f\"reg_loss: {regularization_loss:.3f}), \"\n",
    "                    + f\"lr: {self.optimizer.current_learning_rate}\"\n",
    "                )\n",
    "\n",
    "        if validation_data is not None:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "            output = self.forward(X_val, training=False)\n",
    "\n",
    "            loss = self.loss.calculate(output, y_val)\n",
    "\n",
    "            predictions = self.output_layer_activation.predictions(output)\n",
    "            accuracy = self.accuracy.calculate(predictions, y_val)\n",
    "\n",
    "            print(f\"validation, acc: {accuracy:.3f}, loss: {loss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
