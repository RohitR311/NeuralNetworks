{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb9786c-c25c-4602-a94e-c9b626c922b6",
   "metadata": {},
   "source": [
    "# Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6baca4d2-1930-4d07-8a13-77b0068f1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs\n",
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a14f2f-bc50-482c-bd47-411d088814a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30945e8-c88e-41c3-9533-a6b32e3f17ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269fa7ed-6df0-4a54-8f13-f63bbfd8c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80bf7f61-515c-4cf5-86d2-787083271991",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = X.shape[0] // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b2a88c-944e-42b8-9a22-e84e3917a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps * BATCH_SIZE < X.shape[0]:\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51c7397-9419-4eb9-b10b-d33045f47e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for step in range(steps):\n",
    "        batch_X = X[step * BATCH_SIZE : (step + 1) * BATCH_SIZE]\n",
    "        batch_y = y[step * BATCH_SIZE : (step + 1) * BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c393437-0cc2-4f93-9915-231d36f8f847",
   "metadata": {},
   "source": [
    "## Updates Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20f61c6-031d-4e7f-8a01-088ce3942844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def regularization_loss(self):\n",
    "        regularization_loss = 0\n",
    "\n",
    "        for layer in self.trainable_layers:\n",
    "\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(\n",
    "                    np.abs(layer.weights)\n",
    "                )\n",
    "\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(\n",
    "                    layer.weights * layer.weights\n",
    "                )\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(\n",
    "                    np.abs(layer.biases)\n",
    "                )\n",
    "\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(\n",
    "                    layer.biases * layer.biases\n",
    "                )\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        \n",
    "        return data_loss, self.regularization_loss()\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8259d-72ad-44e6-9617-6435f444bef3",
   "metadata": {},
   "source": [
    "## Updated Accuracy Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91e1a96c-57a6-4a2e-b0fd-48c2ab919b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def calculate(self, predictions, y):\n",
    "        comparisons = self.compare(predictions, y)\n",
    "\n",
    "        accuracy = np.mean(comparisons)\n",
    "        \n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def calculate_accumulated(self):\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e68a97-f913-46e9-bab3-1aa1bb8ecb4a",
   "metadata": {},
   "source": [
    "## Updated Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b9092f-9591-4ec5-8aec-968e4a9b1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.layers[i + 1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i - 1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            if hasattr(self.layers[i], \"weights\"):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "        self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(\n",
    "            self.loss, CategoricalCrossentropy_Loss\n",
    "        ):\n",
    "            self.softmax_classifier_output = (\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "            )\n",
    "\n",
    "    def forward(self, X, training):\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        return layer.output\n",
    "\n",
    "    def backward(self, output, y):\n",
    "        if self.softmax_classifier_output is not None:\n",
    "\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
    "\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "    def train(\n",
    "        self, X, y, *, epochs=1, batch_size=None, print_every=1, validation_data=None\n",
    "    ):\n",
    "\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        train_steps = 1\n",
    "\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            print(f\"epoch: {epoch}\")\n",
    "\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            for step in range(train_steps):\n",
    "\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "\n",
    "                else:\n",
    "                    batch_X = X[step * batch_size : (step + 1) * batch_size]\n",
    "                    batch_y = y[step * batch_size : (step + 1) * batch_size]\n",
    "\n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                data_loss, regularization_loss = self.loss.calculate(\n",
    "                    output, batch_y, include_regularization=True\n",
    "                )\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                predictions = self.output_layer_activation.predictions(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(\n",
    "                        f\"step: {step}, \"\n",
    "                        + f\"acc: {accuracy:.3f}, \"\n",
    "                        + f\"loss: {loss:.3f} (\"\n",
    "                        + f\"data_loss: {data_loss:.3f}, \"\n",
    "                        + f\"reg_loss: {regularization_loss:.3f}), \"\n",
    "                        + f\"lr: {self.optimizer.current_learning_rate}\"\n",
    "                    )\n",
    "\n",
    "            (\n",
    "                epoch_data_loss,\n",
    "                epoch_regularization_loss,\n",
    "            ) = self.loss.calculate_accumulated(include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(\n",
    "                f\"training, \"\n",
    "                + f\"acc: {epoch_accuracy:.3f}, \"\n",
    "                + f\"loss: {epoch_loss:.3f} (\"\n",
    "                + f\"data_loss: {epoch_data_loss:.3f}, \"\n",
    "                + f\"reg_loss: {epoch_regularization_loss:.3f}), \"\n",
    "                + f\"lr: {self.optimizer.current_learning_rate}\"\n",
    "            )\n",
    "\n",
    "            if validation_data is not None:\n",
    "\n",
    "                self.loss.new_pass()\n",
    "                self.accuracy.new_pass()\n",
    "\n",
    "                for step in range(validation_steps):\n",
    "\n",
    "                    if batch_size is None:\n",
    "                        batch_X = X_val\n",
    "                        batch_y = y_val\n",
    "\n",
    "                    else:\n",
    "                        batch_X = X_val[step * batch_size : (step + 1) * batch_size]\n",
    "                        batch_y = y_val[step * batch_size : (step + 1) * batch_size]\n",
    "\n",
    "                    output = self.forward(batch_X, training=False)\n",
    "\n",
    "                    self.loss.calculate(output, batch_y)\n",
    "\n",
    "                    predictions = self.output_layer_activation.predictions(output)\n",
    "\n",
    "                    self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "            validation_loss = self.loss.calculate_accumulated()\n",
    "            validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(f\"validation, acc: {validation_accuracy:.3f}, loss: {validation_loss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
